\documentclass[
    linespread = 1.25
]{ctexart}
\pagestyle{plain}
\ctexset{
    section/format = \Large\bfseries\raggedright,
    section/number = {\chinese{section}、},
    section/aftername = {\enskip},
    abstractname = {\zihao{-2}摘\quad 要}
}

\usepackage[a4paper, lmargin=1in, rmargin=1in, tmargin=1in, bmargin=1in]{geometry}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{graphicx}
\graphicspath{ {./fig} }

\usepackage{listings}
\usepackage{color}

\usepackage[sorting=none]{biblatex}
\addbibresource{LLMTuningReport.bib}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{
  frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=left,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=2
}

\usepackage[hidelinks]{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{siunitx}
\usepackage{algorithm2e}
\SetAlgoInsideSkip{bigskip}
\SetAlgorithmName{算法}{算法}{算法}
\RestyleAlgo{ruled}
\usepackage{multicol}
\usepackage{longtable}
\usepackage{tablefootnote}
\usepackage{float}

\title{\zihao{2}\textbf{大数据创新实践实验报告}\\\zihao{3}\textbf{——多模态大模型LLaVA的微调}}
\author{\zihao{4}曹瀚文 \\\texttt{学号：210810503}
\and \zihao{4}岑畅 \\\texttt{学号：210810501}
\and \zihao{4}丁有罡 \\\texttt{学号：210810518}
\and \zihao{4}符永宣\\\texttt{学号：210810506}
\and \zihao{4}金文韬\\\texttt{学号：210810306}
\and \zihao{4}刘炎培\\\texttt{学号：210810510}
\and \zihao{4}刘梓涛\\\texttt{学号：210810513}
\and \zihao{4}彭珂\\\texttt{学号：210810508}
\and \zihao{4}王子霖\\\texttt{学号：210810522}
\and \zihao{4}文宇祥\\\texttt{学号：210810514}
}
\date{}

\begin{document}

\begin{titlepage}
\newgeometry{top=1in,bottom=1in,right=0.75in,left=0.75in}
\maketitle
\vspace{0.2cm}
\begin{abstract}
  \zihao{-4}
  \vspace{0.8cm}
  \linespread{1.25}
  本论文主要研究了空气质量、污染物水平及其与时空、气候因素的关系，并基于历史数据预测未来空气质量。论文首先对数据进行了预处理，包括数据描述、数据标准化、异常值及缺失值处理、极值值处理等步骤。接着，采用系统聚类方法对不同城市的污染物水平进行了潜在模式探索，通过均值和不同邻距离的聚类方法分析得出了系统聚类结果，并用因子分析的得分对其进行了解释。

  论文进一步探讨了空气质量与时空、气候因素的相关关系，应用多元线性回归模型和改进的多元线性回归模型，诊断模型结果并分析了气候因素与空气质量之间的相关性。此外，论文介绍并应用了广义线性模型，实现利用时空、气候因素对空气质量进行更加准确的预测。
  
  最后，论文利用SARIMA模型和GARCH模型，根据历史空气质量数据预测未来一段时间内的空气质量，以南昌市为例进行了实证研究。通过数据导入、探索性期性、参数确定、模型诊断及预测等步骤，详细展示了两种模型的应用过程和预测效果。
  
  本文不仅揭示了不同城市空气污染物可能存在的潜在模式，同时探究了空气质量与多种因素之间的复杂关系，也为空气质量的预测提供了有效的模型和方法，对城市环境管理、污染控制和空气质量的预报具有重要的参考价值。

  \vspace{1cm}
  \noindent\textbf{关键词：} 系统聚类\hspace{0.22cm} 因子分析\hspace{0.22cm} 多元线性回归\hspace{0.22cm} 广义线性模型\hspace{0.22cm} 时间序列分析
\end{abstract}
\end{titlepage}

\tableofcontents
\newpage
\section{实验背景}
在当今人工智能和机器学习领域，预训练大规模语言模型（如GPT-4）已经展现出卓越的性能。然而，尽管这些模型在许多任务中表现优异，它们的通用性仍然可能无法满足特定应用场景的需求。因此，为了进一步提升模型在特定领域的效果，参数微调成为了一个关键的研究方向。通过微调，我们可以根据特定的数据集和任务需求对模型进行定制，使其在处理特定类型的输入时表现得更加精准和高效。这种方法不仅可以改善模型的预测能力，还能提升其对领域特定知识的理解和应用能力。为了实现最佳的微调效果，研究者们通常需要细致地调整模型的超参数，探索不同的优化策略，以便找到最适合特定应用场景的配置。

本次参数微调实验旨在通过自动驾驶数据集对LLaVA模型进行微调，以期提升该模型在自动驾驶领域的综合表现。同时，在该过程中我们希望探究微调过程中的关键影响因素，并采用多种方法评估微调效果。

本次实验选取的模型是开源的大语言模型LLaVA\cite{liu2023llava}，该模型结合了语言模型和视觉模型，能够同时处理文本和图像输入，从而更好地理解和生成多模态信息，如图\ref{fig:LLaVA}所示。LLaVA通过大规模预训练学习到丰富的特征表示，并在特定任务中通过微调进一步优化性能，在自动驾驶、医疗影像分析、智能客服和内容创作等多个领域展现出广泛的应用前景和高效的推理性能。

本次实验将采用的是LoRA（Low-Rank Adaptation）参数微调算法\cite{hu2021loralowrankadaptationlarge}，它是一种高效的参数微调方法，通过在预训练模型的权重矩阵上添加低秩分解来减少参数数量，如图\ref{fig:LoRA}所示。这种方法不仅显著降低了微调过程中所需的计算资源和存储空间，还能在保持模型性能的同时，快速适应新的任务和数据。LoRA通过只微调一小部分参数，使得大规模预训练模型的应用更加灵活和经济。

\begin{figure}[htbp]
  \centering
  \begin{minipage}[b]{0.6\textwidth}
    \includegraphics[width=\textwidth]{illu_llava.pdf}
    \caption{LLaVA模型架构}
    \label{fig:LLaVA}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.25\textwidth}
    \includegraphics[width=\textwidth]{illu_lora.pdf}
    \caption{LoRA算法图解}
    \label{fig:LoRA}
  \end{minipage}
\end{figure}

% 本次实验在黎勃老师提供的Linux服务器平台上进行，通过SSH远程连接服务器进行实验。

\section{基于自动驾驶数据集的模型微调}

% \subsection{LoRA参数微调算法简介}
% 这段全是GPT写的, 哪个哥们看着改一下吧

% LoRA（Low-Rank Adaptation of Large Language Models）是一种用于微调大型语言模型的算法，旨在减少微调过程中的计算和存储开销。LoRA通过引入低秩矩阵来调整模型的权重，而不是对整个模型进行完全微调。这种方法特别适用于需要在有限资源下进行高效微调的场景。

% \subsubsection{LoRA算法的核心概念}

% \begin{enumerate}
%   \item \textbf{低秩矩阵分解}：
%     \begin{itemize}
%       \item LoRA利用低秩矩阵分解（Low-Rank Decomposition）来表示权重矩阵的变化。具体来说，给定一个权重矩阵$W$，LoRA将其表示为两个低秩矩阵$A$和$B$的乘积，即$W + \Delta W \approx W + A \cdot B$，其中$\Delta W$是权重的调整部分。
%     \end{itemize}
%   \item \textbf{参数高效性}：
%     \begin{itemize}
%       \item 通过使用低秩矩阵，LoRA显著减少了需要学习的参数数量。这不仅降低了存储需求，还减少了训练和推理的计算开销。
%     \end{itemize}
%   \item \textbf{保持原模型的冻结状态}：
%     \begin{itemize}
%       \item 在LoRA中，原始模型的权重保持冻结状态，只有低秩矩阵$A$和$B$会在微调过程中进行更新。这使得LoRA在微调过程中能够更好地保持原始模型的特性，同时适应新的任务需求。
%     \end{itemize}
% \end{enumerate}

% \subsubsection{LoRA的工作流程}

% \begin{enumerate}
%   \item \textbf{初始化低秩矩阵}：
%     \begin{itemize}
%       \item 在开始微调之前，LoRA初始化两个低秩矩阵$A$和$B$，它们的维度通常远小于原始权重矩阵$W$。
%     \end{itemize}
%   \item \textbf{微调过程}：
%     \begin{itemize}
%       \item 在微调过程中，仅对低秩矩阵$A$和$B$进行更新，原始权重矩阵$W$保持不变。通过梯度下降法，优化目标函数以最小化模型在新任务上的损失。
%     \end{itemize}
%   \item \textbf{预测和推理}：
%     \begin{itemize}
%       \item 在推理阶段，将更新后的低秩矩阵与原始权重矩阵结合，生成最终的权重$W' = W + A \cdot B$，用于模型预测。
%     \end{itemize}
% \end{enumerate}

% \subsubsection{LoRA的优点}

% \begin{enumerate}
%   \item \textbf{参数效率}：
%     \begin{itemize}
%       \item LoRA通过引入低秩矩阵显著减少了需要更新的参数数量，使得微调过程更为高效。
%     \end{itemize}
%   \item \textbf{资源节省}：
%     \begin{itemize}
%       \item 由于只需更新较小的低秩矩阵，LoRA在存储和计算资源上的需求较低，非常适合在资源受限的环境中进行微调。
%     \end{itemize}
%   \item \textbf{保持原始模型特性}：
%     \begin{itemize}
%       \item LoRA在微调过程中冻结了原始模型的权重，这有助于保持原始模型的特性和性能。
%     \end{itemize}
% \end{enumerate}

% \subsubsection{LoRA在实践中的应用}

% LoRA已经在多个自然语言处理（NLP）任务中得到了应用，例如文本分类、情感分析和机器翻译。它特别适用于大规模预训练模型的微调，例如BERT、GPT和T5等。

% 通过使用LoRA，开发者可以在保持模型高性能的同时，大幅减少微调所需的计算和存储资源，使得大模型的应用更加广泛和便捷。

\subsection{使用LoRA算法和自动驾驶数据集对LLaVA模型作指令微调}
我们对llava-v1.5-7b和llava-v1.5-13b两个模型使用了LoRA算法进行微调，两个模型的区别在于规模大小，其中7b模型的参数量为70亿，13b模型的参数量为130亿。

我们使用的数据集是\texttt{bdd1k\_by\_Qiancai}。该数据集由王前才学长提供，衍生自开源的自动驾驶图片数据集\texttt{BDD100k}\cite{yu2020bdd100kdiversedrivingdataset}，从中选取的1000张图片尽可能地覆盖了各种驾驶场景，
并借助商业多模态模型辅助了生成图像-文本对数据集，经人工检查保证了数据集质量。

我们使用的微调方法是LLaVA官方提供的微调脚本。为使微调效果更佳，我们通过尝试修改了一些训练参数。对上述两个模型分别进行了10个epoch的微调，微调过程中的损失函数变化如图\ref{fig:7b规模模型训练过程中的loss}、\ref{fig:13b规模模型训练过程中的loss}所示。可以看到，随着模型微调轮数的增加，模型的损失函数逐渐下降，表明模型在训练过程中逐渐收敛。
\begin{figure}[H]
  \centering
  \begin{subfigure}{0.49\textwidth}
    \includegraphics[width=\textwidth]{7b.png}
    \caption{7b规模模型}
    \label{fig:7b规模模型训练过程中的loss}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.49\textwidth}
    \includegraphics[width=\textwidth]{13b.png}
    \caption{13b规模模型}
    \label{fig:13b规模模型训练过程中的loss}
  \end{subfigure}
  \caption{模型训练过程中的损失函数变化}
  \label{fig:模型训练过程中的loss}
\end{figure}


\subsection{实现微调模型的图形化界面}
开源的LLaVA模型提供的交互接口较为原始，每一步调用都需要通过命令行传递模型的路径、训练结果的路径等相关参数，且难以实现连续的对话。

为了方便模型的后续调用与测试，我们对参数微调后的LLaVA模型进行了适当封装，并借助Python的curses库制作了一个朴素的GUI界面进行模型的调用，如图\ref{fig:gui_result}所示。用户可以通过该界面选择使用的模型微调版本、输入图片路径，并与模型进行连续对话，如图\ref{fig:result}所示。

通过GUI界面图\ref{fig:gui}可以清楚地看到，模型在我们给出的示例图片上的回答可以令人满意。
\begin{figure}[htbp]
  \centering
  \begin{subfigure}{0.6\textwidth}
    \centering
    \includegraphics[width=\textwidth]{gui.png}
    \caption{GUI界面}
    \label{fig:gui}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.36\textwidth}
    \includegraphics[width=\textwidth]{0020.jpg}
    \caption{示例图片\texttt{0020.jpg}}
    \label{fig:result}
  \end{subfigure}
  \caption{GUI界面运行示例}
  \label{fig:gui_result}
\end{figure}

\subsection{结论}

\section{探索影响微调效果的实验因素}

基于网络上搜集的行车记录仪图片*以及对于以下6个问题的问答，初步探索了训练周期与模型大小对微调效果的影响，结果见表格。

\begin{enumerate} 
    \item Can you describe the general scene?（回答是否宽泛）
    \item Who are you?（能否以机器人身份回答）
    \item Can I speed up, in this situation?（能否给出精确答案）
    \item Can I change lanes, in this situation?（能否给出精确答案）
    \item Give the number of cars in the scene.
    \item Are there any buses on the road?
\end{enumerate}

\subsection{探究训练周期对微调效果的影响}

\begin{table}[ht]
\centering
\caption{训练周期对微调效果的影响}%标题
\begin{tabular}{ccccccc}
\Xhline{2pt}
问题 & 描述图片 & 询问身份 & 能否加速 & 能否变道 & 汽车数量 & 是否有公交 \\
\Xhline{1pt}
13b_0epoch & 回答宽泛 & 机器人 & 回答宽泛 & 回答宽泛 & 5 & 无 \\
13b_10epoch & 强调道路 & 机器人 & 回答精确(No,...) & 回答精确(Yes,...) & 6 & 无 \\
\Xhline{2pt}
\end{tabular}
\label{effect_epoch}
\end{table}

\subsection{探究模型大小对微调效果的影响}

\begin{table}[ht]
\centering
\caption{模型大小对微调效果的影响}%标题
\begin{tabular}{ccccccc}
\Xhline{2pt}
问题 & 描述图片 & 询问身份 & 能否加速 & 能否变道 & 汽车数量 & 是否有公交 \\
\Xhline{1pt}
7b_10epoch & 强调道路 & textbf{驾驶者} & 回答精确(No,...) & textbf{回答宽泛} & 6 & 无 \\
13b_10epoch & 强调道路 & 机器人 & 回答精确(No,...) & 回答精确(Yes,...) & 6 & 无 \\
\Xhline{2pt}
\end{tabular}
\label{effect_size}
\end{table}

\subsection{结论}

经过以上比较，可以发现：

\begin{enumerate} 
    \item 适当增加训练epoch，模型的回答更强调道路，更加精简，而非宽泛的回答；对于驾驶问题能够先给出精确回答（Yes/No），而非给出建议再让提问者抉择，而这正是自动驾驶模型所需要的判断能力；另外，训练后的模型有更好的视力，可以看到旁侧车道的远方车辆。
    \item 模型大小对于效果的影响主要在于，7b模型的微调模型仍会以驾驶者身份回答问题，而非所期望的机器人身份。
\end{enumerate}


\section{微调结果的分析与评估}

\subsection{使用维基百科数据集进行主观初步评估}

\subsection{计算微调模型的相关评价指标}

\subsection{使用Coda实现模型在自动驾驶数据集上的初步评估}

\subsection{结论}

\section{实验结论}

\appendix
\newpage
\section*{参考文献}
\addcontentsline{toc}{section}{参考文献}
\printbibliography[heading=none]
% \noindent
% [1] 乐东明,王文浚,王颖,等. 2020—2022年咸宁市臭氧污染气象特征及成因分析[J].黑龙江环境通报, 2024, 37(05):30-32.

% \noindent
% [2] 李高荣,吴密霞. 多元统计分析[M]. 北京：科学出版社, 2021

% \noindent
% [3] John A. Rice. Mathematical Statistics and Data Analysis[M]. Boston: Cengage Learning, 2006

% \noindent
% [4] 何书元. 应用时间序列分析[M]. 北京：北京大学出版社, 2003

\newpage
\section*{附录：实验日志与心得}
\addcontentsline{toc}{section}{附录：实验日志与心得}

\end{document}
